---
layout: post
title: "Attention Is All You Need"
category: Daily Paper
tag: 
---

> **Daily Paper**

하루에 한 논문을 30분 안에 간단하게 살펴본 결과를 기록하는 포스트입니다.

논문을 살펴볼 때 ChatGPT의 도움을 적극적으로 받으며, 따라서 포스트에 잘못된 내용이나 오류가 있을 수 있습니다.

피드백이나 의견 있으시면 언제든지 [연락](/about)주세요.

---
# 3줄 요약
- LLM은 다양한 문제를 훌륭하게 해결하지만, 자신의 답변에서 오류를 스스로 인식하고 수정하는 능력은 상대적으로 약함
- 학습 과정을 2단계(답변 생성 -> 답변 수정)으로 나누어서 학습하게 하여 자기수정(self-correct) 능력을 크게 향상
- 인간의 사고과정을 점점 따라가고 있는 듯한 느낌

---

# 논문 정보
- 논문 제목: Attention Is All You Need
- 저자: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
- 소속: Google Brain, Google Research, University of Toronto
- 원문 링크: [arXiv 링크](https://arxiv.org/abs/2409.12917)

---

# 배경
- LLM은 다양한 문제를 훌륭하게 해결하지만, 그 과정에서 발생하는 <u>오류를 스스로 인식하고 수정하는 능력은 상대적으로 약한 상태</u>
  - 수학적 추론, 코딩, 논리적 문제 해결과 같은 영역에서 매우 중요
- 기존에는 아래와 같은 시도들이 있었으나, 효과적이지 않았음
  - 프롬프트 엔지니어링 : 의미있는 변화가 없었음
  - 파인 튜닝 : 입력을 처리하는 별도 모델이 필요하거나, 수정 과정을 감독하는 teacher 모델이 필요

# 문제
- **Distribution Shift (분포 불일치)**
  - 훈련 데이터와 실제 문제 해결 중 발생하는 오류의 차이로 인해 성능이 저하
- **Behavior Collapse (행동 붕괴)**
  - 학습이 첫 번째 시도의 최적 응답을 생성하는 데 집중되고 두 번째 시도에서는 피상적인 수정 또는 수정이 전혀 이루어지지 않는 경우

# SCoRe
- <u>S</u>elf-<u>Co</u>rrection via <u>Re</u>inforcement Learning
- 대규모 언어 모델(LLM)이 외부 감독 없이 스스로 오류를 수정할 수 있도록 강화 학습(RL)을 사용한 새로운 접근법

## 학습 단계
![image](/assets/2024-10-22-daily_paper-6-2409.12917_1.png)


### 1단계: 단계별로 상관관계를 최소화하는 초기화 과정을 학습
- 기존 모델의 응답에서 너무 많이 동떨어지지 않으면서 2단계에서 잘 고칠 수 있는 답변을 생성하는 단계
  - 비효율적이게 보일 수 있지만, 행동 붕괴(behavior collapse)가 발생하지 않도록 하기 위해서 꼭 필요함
  - KL-발산(KL-divergence)을 이용하여 조정

### 2단계: 보상 형성을 적용한 다단계 강화 학습
- 1단계의 결과에서부터 시작하여 1단계와 2단계를 모두 최적화 함
- 강화학습의 보상에 단순히 정답과 가까운 정도 만을 보상으로 제공하는 것이 아니라, 1단계의 답변을 수정한 정도를 추가함
  - ![image](/assets/2024-10-22-daily_paper-6-2409.12917_2.png)
    - 잘못된 1단계 답변을 제대로 고치면 양(+)의 보상
    - 올바른 1단계 답변을 틀리게 고치면 음(-)의 보상

### 기타
- 필요시 위 단계를 반복하면서 Multi-Trun 학습
- 답변을 실시간으로 생성하며 온라인(online) 학습

## 특징
- 단순한 단일 모델 아키텍처
  - 단일 모델이 자체적으로 오답 데이터를 생성하고 해당 문제를 오류를 인식하고 수정할 수 있도록 훈련
- 다양한 오답 데이터셋을 자체 생성하면서 분포 불일치 문제 해결
- 두 단계로 분리하여 학습흐며 행동 붕괴 방지
- 단순히 정답을 학습하는 것이 아닌 오류를 인식하고 수정하는 능력을 학습

## 성과
- 수학 및 코딩 문제에서 우수한 성능
  - MATH 문제에서 15.6%, HumanEval 코딩 문제에서 9.1%의 자기 수정 성능 향상을 기록

---

# Abstract

대규모 언어 모델(LLM)의 자기 수정(self-correction) 능력은 매우 바람직한 특성이지만, 현대의 LLM에서는 이러한 능력이 대체로 효과적이지 않다는 것이 반복적으로 밝혀졌습니다. 현재 자기 수정 능력을 훈련하는 방법들은 여러 모델을 사용하거나, 더 진보된 모델, 또는 추가적인 감독 형태에 의존하고 있습니다. 이러한 단점을 해결하기 위해, 우리는 오직 자체 생성 데이터만을 사용하여 LLM의 자기 수정 능력을 크게 향상시키는 다단계 온라인 강화 학습(RL) 접근법인 SCoRe를 개발했습니다. SCoRe를 구축하기 위해, 우리는 오프라인 모델이 생성한 수정 경로를 사용한 변형된 지도 학습 방식(SFT)이 자기 수정 행동을 주입하기에 충분하지 않다는 것을 먼저 보여줍니다. 특히, SFT 방식은 데이터 수집 정책이 만든 실수와 모델 자체의 응답 간의 분포 불일치로 인해 제대로 작동하지 않거나, 학습이 특정 수정 행동 모드에만 치우쳐 테스트 문제에서 자기 수정이 효과적이지 못한 경우가 발생합니다. SCoRe는 모델이 생성한 자체 수정 경로에서 학습하여 이러한 문제를 해결하고, 테스트 시에 효과적인 자기 수정 행동을 학습하도록 적절한 규제를 적용합니다. 이 규제 과정에는 행동 붕괴를 방지하기 위한 기본 모델에 대한 다단계 RL 초기화가 포함되며, 자기 수정을 증폭시키기 위한 보상 추가가 이어집니다. 우리는 Gemini 1.0 Pro 및 1.5 Flash 모델을 사용하여, SCoRe가 수학(MATH) 및 코딩(HumanEval) 문제에서 각각 15.6%와 9.1%의 성능 향상을 기록하며, 최첨단 자기 수정 성능을 달성했음을 확인했습니다.

---

# 목차 및 요약

## 1. Introduction
- Transformer 모델을 통해 시퀀스 변환 작업에서 복잡한 순환 구조를 대체할 수 있음을 제안합니다.
- 이 모델은 주의(attention) 메커니즘만으로 구성되어 병렬 처리가 용이하고 계산 효율성이 높습니다.

## 2. Background
- 기존의 시퀀스 변환 모델은 RNN과 CNN 기반으로 동작했으며, 각 모델의 특징과 한계를 논의합니다.
- 특히, RNN의 장기 의존성 문제와 CNN의 계산 복잡성을 해결하기 위한 방안으로 Transformer가 제시됩니다.

## 3. Model Architecture
- Transformer의 아키텍처는 완전한 Self-Attention 메커니즘을 기반으로 하며, 인코더와 디코더 스택으로 구성됩니다.
- 각 구성 요소는 주의 메커니즘을 중심으로 하여 다양한 시퀀스 변환 작업에 적합하도록 설계되었습니다.

### 3.1 Encoder and Decoder Stacks
- Transformer는 총 6개의 인코더와 6개의 디코더 층을 스택 형태로 배치한 구조입니다.
- 각 인코더는 Self-Attention과 Position-wise Feed-Forward 네트워크로 구성되며, 디코더는 인코더의 출력에 기반한 Masked Self-Attention을 추가로 포함합니다.

### 3.2 Attention
- Attention 메커니즘은 Self-Attention을 통해 문장 내 단어 간의 상관관계를 학습합니다.
- 각 단어는 Query, Key, Value로 표현되어 유사도와 중요도를 계산합니다.

#### 3.2.1 Scaled Dot-Product Attention
- Scaled Dot-Product Attention은 Query와 Key 간의 내적을 통해 유사도를 측정하고, 각 Value에 가중치를 할당하여 최종 출력을 생성합니다.
- 여기서 내적 값을 스케일링하여 안정적 학습을 보장합니다.

#### 3.2.2 Multi-Head Attention
- Multi-Head Attention은 Self-Attention을 병렬로 수행하여 다양한 관계 정보를 추출합니다.
- 이는 모델이 여러 해석 가능성을 고려하게 하여 더 풍부한 의미 파악이 가능하게 합니다.

### 3.3 Position-wise Feed-Forward Networks
- 각 인코더와 디코더 층에서 Attention의 출력을 추가로 처리하기 위한 Position-wise Feed-Forward 네트워크입니다.
- 각 위치에서 독립적으로 동일한 변환을 수행하여 단어 간의 정보 결합을 강화합니다.

### 3.4 Positional Encoding
- Transformer는 순차적인 구조가 없기 때문에, 입력에 위치 정보를 포함하기 위해 Positional Encoding을 사용합니다.
- 이 인코딩은 각 단어의 위치를 시계열 정보로 모델에 전달하여 순서의 개념을 학습하게 합니다.

## 4. Why Self-Attention
- Self-Attention의 병렬화 가능성과 효율성에 대해 논의합니다.
- 이 기법은 긴 시퀀스에서도 장기 의존성을 효과적으로 학습할 수 있습니다.

## 5. Training
- 모델 훈련에서 필요한 최적화, 하이퍼파라미터 설정 및 학습 방법을 설명합니다.
- 이 절에서는 훈련 시의 주요 고려사항과 정규화 방법도 소개합니다.

### 5.1 Optimizer
- Adam 최적화 알고리즘을 사용하여 모델 학습을 최적화합니다.
- 이는 학습 속도를 높이고 안정성을 증가시킵니다.

### 5.2 Regularization
- 드롭아웃(Dropout)과 같은 정규화 기법을 사용하여 과적합을 방지합니다.
- 정규화는 모델의 일반화 성능을 향상시킵니다.

## 6. Results
- Transformer 모델의 성능을 다양한 언어 번역 데이터셋을 통해 평가합니다.
- Transformer는 기존 모델을 능가하는 성능을 보였으며, 특히 계산 효율성에서 우수함을 입증했습니다.

### 6.1 Machine Translation
- WMT 2014 영어-독일어 및 영어-프랑스어 데이터셋에서 Transformer가 RNN 및 CNN 기반 모델을 뛰어넘는 성능을 보였습니다.

## 7. Conclusion
- Transformer 모델이 RNN 및 CNN 구조의 대안으로서 높은 성능을 보여줍니다.
- 이 모델은 병렬 처리에 유리하고, 다양한 시퀀스 변환 작업에서 유용하게 활용될 수 있습니다.
- 
---
